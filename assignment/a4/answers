# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 1 parts for a total of 27 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Image Captioning (27 points)



###################################################################
###################################################################
## Image Captioning (27 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (A): Data Exploration (12 points)  | 
# ------------------------------------------------------------------

# Question 1 (/4): How many images are in the training set?
image_captioning_a_1: 
- your answer

# Question 2 (/4): To the nearest integer, how many captions are there per image in the training set?
image_captioning_a_2: 
- your answer

# Question 3 (/4): How many unique tokens are there after splitting captions on whitespace only in the training set?
image_captioning_a_3: 
- your answer


# ------------------------------------------------------------------
# | Section (B): Show and Tell (5 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): Which parts of the CNN were fine-tuned during the image caption generation process?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_1: 
 - The kernels within the CNN
 - The top feed forward layer of the CNN
 - All of the above

# Question 2 (/1): What was the biggest concern when deciding how to train the model?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_2: 
 - Training time
 - Overfitting

# Question 3 (/1): How was the encoded image representation input into the decoder?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_3: 
 - As an image vector at each time step
 - As the first word embedding input

# Question 4 (/1): Which metric did the authors use to determine success?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_4: 
 - BLEU
 - ROUGE
 - METEOR

# Question 5 (/1): What beam width is equivalent to selecting the highest probability word in each decoding step?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_b_5: 
 - 0
 - 1
 - infinite


# ------------------------------------------------------------------
# | Section (C): Show, Tell and Attend (2 points)  | 
# ------------------------------------------------------------------

# Question 1 (/1): What is the model paying attention to?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_c_1: 
 - The raw pixels
 - Multiple independently trained CNNs over an object classification task, each yielding an annotation
 - Vectors are extracted from a layer of the CNN with a receptive field (image region contributing to it) smaller than the full image

# Question 2 (/1): What do the figures with highlight shading represent in Figures 2, 3 and 5?
# (This question is multiple choice.  Delete all but the correct answer).
image_captioning_c_2: 
 - The part of the image contributing to the word currently being decoded
 - The part of the image most contributing to the generated caption


# ------------------------------------------------------------------
# | Section (D): CLIP (8 points)  | 
# ------------------------------------------------------------------

# Question 1 (/2): What is the animal tag you selected?
image_captioning_d_1: your answer

# Question 2 (/2): What is the transportation tag you selected?
image_captioning_d_2: your answer

# Question 3 (/2): What is the probability associated with the most likely caption for image 1?
image_captioning_d_3: 0.00000

# Question 4 (/2): What is the probability associated with the most likely caption for image 2?
image_captioning_d_4: 0.00000
